{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"FpTesting_croppingTransforming.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMOezraZW2yvMKFQaHoF/0V"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0ffd70aa02d54102a8a86b5c6dee3b08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b3b8bd5e20f343f6b957c015c3e7a397","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_58096c7220494b4799e7e179ef8775af","IPY_MODEL_f06d9899c18d42a8ac3e3713602fa87b"]}},"b3b8bd5e20f343f6b957c015c3e7a397":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"58096c7220494b4799e7e179ef8775af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d7c05acfc1b54839b0c4c9bd06af72bf","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"danger","max":6,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_39d109915f1a441a8666ab6e2d175eff"}},"f06d9899c18d42a8ac3e3713602fa87b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_398723bac15540da844d922e0356fbe7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/6 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a85fa578b47f4ed78955d09ccb11b194"}},"d7c05acfc1b54839b0c4c9bd06af72bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"39d109915f1a441a8666ab6e2d175eff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"398723bac15540da844d922e0356fbe7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a85fa578b47f4ed78955d09ccb11b194":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qn5kPd4lrp_2","executionInfo":{"status":"ok","timestamp":1617523836801,"user_tz":-330,"elapsed":38295,"user":{"displayName":"VAIDEHI SOM","photoUrl":"","userId":"04027413623674072385"}},"outputId":"446cc617-d82c-4cd9-a7e2-a3e83172058c"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UUJeyeP8rqlX"},"source":["import os\n","os.chdir('/content/drive/My Drive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TpQ1e3nTsby1"},"source":["import torch\n","from torch import nn\n","from tqdm.auto import tqdm\n","from torchvision import transforms\n","from torchvision.utils import make_grid, save_image\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","#torch.manual_seed(0)\n","\n","def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n","    '''\n","    Function for visualizing images: Given a tensor of images, number of images, and\n","    size per image, plots and prints the images in an uniform grid.\n","    '''\n","    image_tensor = (image_tensor + 1) / 2\n","    image_shifted = image_tensor\n","    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n","    #img = image_unflat[:num_images]\n","    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n","    #plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n","    plt.imshow(image_grid.permute(1,2,0).squeeze())\n","    #save_image(image_unflat[:num_images], 'real1.jpg')\n","    #plt.imshow(image_unflat[:num_images].permute(1, 2, 0).squeeze())\n","    #plt.imshow(image_unflat[:num_images].permute(1, 2, 0).squeeze())\n","    plt.show()\n","    return(image_unflat[:num_images])\n","\n","import glob\n","import random\n","import os\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","class ImageDataset(Dataset):\n","    def __init__(self, root, transform=None, mode='test'):\n","        self.transform = transform\n","        self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + '/67_*.*'))\n","        \n","        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + '/67_*.*'))\n","        \n","        if len(self.files_A) >= len(self.files_B):\n","            self.files_A, self.files_B = self.files_B, self.files_A\n","        self.new_perm()\n","        assert len(self.files_A) > 0\n","\n","    def new_perm(self):\n","        self.randperm = torch.randperm(len(self.files_B))[:len(self.files_A)]\n","\n","    def __getitem__(self, index):\n","        #print(len(self.files_A))\n","        #print(len(self.files_B))\n","        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n","        item_B = self.transform(Image.open(self.files_B[self.randperm[index]]))\n","        if item_A.shape[0] != 3: \n","            item_A = item_A.repeat(3, 1, 1)\n","        if item_B.shape[0] != 3: \n","            item_B = item_B.repeat(3, 1, 1)\n","        if index == len(self) - 1:\n","            self.new_perm()\n","        # Old versions of PyTorch didn't support normalization for different-channeled images\n","        return (item_A - 0.5) * 2, (item_B - 0.5) * 2\n","\n","    def __len__(self):\n","        return len(self.files_A)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPk6sjxokamn"},"source":["class ResidualBlock(nn.Module):\n","    '''\n","    ResidualBlock Class:\n","    Performs two convolutions and an instance normalization, the input is added\n","    to this output to form the residual block output.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def __init__(self, input_channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, padding_mode='reflect')\n","        self.conv2 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, padding_mode='reflect')\n","        self.instancenorm = nn.InstanceNorm2d(input_channels)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of ResidualBlock: \n","        Given an image tensor, completes a residual block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        original_x = x.clone()\n","        x = self.conv1(x)\n","        x = self.instancenorm(x)\n","        x = self.activation(x)\n","        x = self.conv2(x)\n","        x = self.instancenorm(x)\n","        return original_x + x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEIui2dlkbPU"},"source":["class ContractingBlock(nn.Module):\n","    '''\n","    ContractingBlock Class\n","    Performs a convolution followed by a max pool operation and an optional instance norm.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def __init__(self, input_channels, use_bn=True, kernel_size=3, activation='relu'):\n","        super(ContractingBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(input_channels, input_channels * 2, kernel_size=kernel_size, padding=1, stride=2, padding_mode='reflect')\n","        self.activation = nn.ReLU() if activation == 'relu' else nn.LeakyReLU(0.2)\n","        if use_bn:\n","            self.instancenorm = nn.InstanceNorm2d(input_channels * 2)\n","        self.use_bn = use_bn\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of ContractingBlock: \n","        Given an image tensor, completes a contracting block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x = self.conv1(x)\n","        if self.use_bn:\n","            x = self.instancenorm(x)\n","        x = self.activation(x)\n","        return x\n","\n","class ExpandingBlock(nn.Module):\n","    '''\n","    ExpandingBlock Class:\n","    Performs a convolutional transpose operation in order to upsample, \n","        with an optional instance norm\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def __init__(self, input_channels, use_bn=True):\n","        super(ExpandingBlock, self).__init__()\n","        self.conv1 = nn.ConvTranspose2d(input_channels, input_channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n","        if use_bn:\n","            self.instancenorm = nn.InstanceNorm2d(input_channels // 2)\n","        self.use_bn = use_bn\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of ExpandingBlock: \n","        Given an image tensor, completes an expanding block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","            skip_con_x: the image tensor from the contracting path (from the opposing block of x)\n","                    for the skip connection\n","        '''\n","        x = self.conv1(x)\n","        if self.use_bn:\n","            x = self.instancenorm(x)\n","        x = self.activation(x)\n","        return x\n","\n","class FeatureMapBlock(nn.Module):\n","    '''\n","    FeatureMapBlock Class\n","    The final layer of a Generator - \n","    maps each the output to the desired number of output channels\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","        output_channels: the number of channels to expect for a given output\n","    '''\n","    def __init__(self, input_channels, output_channels):\n","        super(FeatureMapBlock, self).__init__()\n","        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=7, padding=3, padding_mode='reflect')\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of FeatureMapBlock: \n","        Given an image tensor, returns it mapped to the desired number of channels.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x = self.conv(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqMwKapnkbi8"},"source":["class Generator(nn.Module):\n","    '''\n","    Generator Class\n","    A series of 2 contracting blocks, 9 residual blocks, and 2 expanding blocks to \n","    transform an input image into an image from the other class, with an upfeature\n","    layer at the start and a downfeature layer at the end.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","        output_channels: the number of channels to expect for a given output\n","    '''\n","    def __init__(self, input_channels, output_channels, hidden_channels=64):\n","        super(Generator, self).__init__()\n","        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n","        self.contract1 = ContractingBlock(hidden_channels)\n","        self.contract2 = ContractingBlock(hidden_channels * 2)\n","        res_mult = 4\n","        self.res0 = ResidualBlock(hidden_channels * res_mult)\n","        self.res1 = ResidualBlock(hidden_channels * res_mult)\n","        self.res2 = ResidualBlock(hidden_channels * res_mult)\n","        self.res3 = ResidualBlock(hidden_channels * res_mult)\n","        self.res4 = ResidualBlock(hidden_channels * res_mult)\n","        self.res5 = ResidualBlock(hidden_channels * res_mult)\n","        self.res6 = ResidualBlock(hidden_channels * res_mult)\n","        self.res7 = ResidualBlock(hidden_channels * res_mult)\n","        self.res8 = ResidualBlock(hidden_channels * res_mult)\n","        self.expand2 = ExpandingBlock(hidden_channels * 4)\n","        self.expand3 = ExpandingBlock(hidden_channels * 2)\n","        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n","        self.tanh = torch.nn.Tanh()\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of Generator: \n","        Given an image tensor, passes it through the U-Net with residual blocks\n","        and returns the output.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x0 = self.upfeature(x)\n","        x1 = self.contract1(x0)\n","        x2 = self.contract2(x1)\n","        x3 = self.res0(x2)\n","        x4 = self.res1(x3)\n","        x5 = self.res2(x4)\n","        x6 = self.res3(x5)\n","        x7 = self.res4(x6)\n","        x8 = self.res5(x7)\n","        x9 = self.res6(x8)\n","        x10 = self.res7(x9)\n","        x11 = self.res8(x10)\n","        x12 = self.expand2(x11)\n","        x13 = self.expand3(x12)\n","        xn = self.downfeature(x13)\n","        return self.tanh(xn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkSDXSxLkbw8"},"source":["class Discriminator(nn.Module):\n","    '''\n","    Discriminator Class\n","    Structured like the contracting path of the U-Net, the discriminator will\n","    output a matrix of values classifying corresponding portions of the image as real or fake. \n","    Parameters:\n","        input_channels: the number of image input channels\n","        hidden_channels: the initial number of discriminator convolutional filters\n","    '''\n","    def __init__(self, input_channels, hidden_channels=64):\n","        super(Discriminator, self).__init__()\n","        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n","        self.contract1 = ContractingBlock(hidden_channels, use_bn=False, kernel_size=4, activation='lrelu')\n","        self.contract2 = ContractingBlock(hidden_channels * 2, kernel_size=4, activation='lrelu')\n","        self.contract3 = ContractingBlock(hidden_channels * 4, kernel_size=4, activation='lrelu')\n","        self.final = nn.Conv2d(hidden_channels * 8, 1, kernel_size=1)\n","\n","    def forward(self, x):\n","        x0 = self.upfeature(x)\n","        x1 = self.contract1(x0)\n","        x2 = self.contract2(x1)\n","        x3 = self.contract3(x2)\n","        xn = self.final(x3)\n","        return xn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mgFxQF9kcAN"},"source":["import torch.nn.functional as F\n","\n","adv_criterion = nn.MSELoss() \n","recon_criterion = nn.L1Loss() \n","\n","n_epochs = 1\n","dim_A = 3\n","dim_B = 3\n","display_step = 1\n","batch_size = 1\n","lr = 0.0002\n","load_shape = (356, 328) #(800, 552)  #(400,275) #286\n","target_shape1 = 160\n","target_shape2 = 144\n","device = 'cuda'  #cpu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RovBq6hdkmGG"},"source":["transform = transforms.Compose([\n","    transforms.Resize(load_shape),\n","    transforms.CenterCrop(size=(target_shape1, target_shape2)),\n","    #transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","])\n","\n","import torchvision\n","dataset = ImageDataset(\"data_5\", transform=transform)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAxndeWkkmU9"},"source":["gen_AB = Generator(dim_A, dim_B).to(device)\n","gen_BA = Generator(dim_B, dim_A).to(device)\n","gen_opt = torch.optim.Adam(list(gen_AB.parameters()) + list(gen_BA.parameters()), lr=lr, betas=(0.5, 0.999))\n","disc_A = Discriminator(dim_A).to(device)\n","disc_A_opt = torch.optim.Adam(disc_A.parameters(), lr=lr, betas=(0.5, 0.999))\n","disc_B = Discriminator(dim_B).to(device)\n","disc_B_opt = torch.optim.Adam(disc_B.parameters(), lr=lr, betas=(0.5, 0.999))\n","\n","\n","\n","pre_dict = torch.load('cycleGAN_3_23600.pth')\n","gen_AB.load_state_dict(pre_dict['gen_AB'])\n","gen_BA.load_state_dict(pre_dict['gen_BA'])\n","gen_opt.load_state_dict(pre_dict['gen_opt'])\n","disc_A.load_state_dict(pre_dict['disc_A'])\n","disc_A_opt.load_state_dict(pre_dict['disc_A_opt'])\n","disc_B.load_state_dict(pre_dict['disc_B'])\n","disc_B_opt.load_state_dict(pre_dict['disc_B_opt'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WpKE6E91kmnd"},"source":["def get_disc_loss(real_X, fake_X, disc_X, adv_criterion):\n","    '''\n","    Return the loss of the discriminator given inputs.\n","    Parameters:\n","        real_X: the real images from pile X\n","        fake_X: the generated images of class X\n","        disc_X: the discriminator for class X; takes images and returns real/fake class X\n","            prediction matrices\n","        adv_criterion: the adversarial loss function; takes the discriminator \n","            predictions and the target labels and returns a adversarial \n","            loss (which you aim to minimize)\n","    '''\n","    disc_fake_X_hat = disc_X(fake_X.detach()) # Detach generator\n","    disc_fake_X_loss = adv_criterion(disc_fake_X_hat, torch.zeros_like(disc_fake_X_hat))\n","    disc_real_X_hat = disc_X(real_X)\n","    disc_real_X_loss = adv_criterion(disc_real_X_hat, torch.ones_like(disc_real_X_hat))\n","    disc_loss = (disc_fake_X_loss + disc_real_X_loss) / 2\n","    return disc_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLQLRsyXk_A1"},"source":["def get_gen_adversarial_loss(real_X, disc_Y, gen_XY, adv_criterion):\n","    '''\n","    Return the adversarial loss of the generator given inputs\n","    (and the generated images for testing purposes).\n","    Parameters:\n","        real_X: the real images from pile X\n","        disc_Y: the discriminator for class Y; takes images and returns real/fake class Y\n","            prediction matrices\n","        gen_XY: the generator for class X to Y; takes images and returns the images \n","            transformed to class Y\n","        adv_criterion: the adversarial loss function; takes the discriminator \n","                  predictions and the target labels and returns a adversarial \n","                  loss (which you aim to minimize)\n","    '''\n","    fake_Y = gen_XY(real_X)\n","    disc_fake_Y_hat = disc_Y(fake_Y)\n","    adversarial_loss = adv_criterion(disc_fake_Y_hat, torch.ones_like(disc_fake_Y_hat))\n","    return adversarial_loss, fake_Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6NifNoZk_Ok"},"source":["def get_identity_loss(real_X, gen_YX, identity_criterion):\n","    '''\n","    Return the identity loss of the generator given inputs\n","    (and the generated images for testing purposes).\n","    Parameters:\n","        real_X: the real images from pile X\n","        gen_YX: the generator for class Y to X; takes images and returns the images \n","            transformed to class X\n","        identity_criterion: the identity loss function; takes the real images from X and\n","                        those images put through a Y->X generator and returns the identity \n","                        loss (which you aim to minimize)\n","    '''\n","    identity_X = gen_YX(real_X)\n","    identity_loss = identity_criterion(identity_X, real_X)\n","    return identity_loss, identity_X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yy2KkpIpk_Z0"},"source":["def get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion):\n","    '''\n","    Return the cycle consistency loss of the generator given inputs\n","    (and the generated images for testing purposes).\n","    Parameters:\n","        real_X: the real images from pile X\n","        fake_Y: the generated images of class Y\n","        gen_YX: the generator for class Y to X; takes images and returns the images \n","            transformed to class X\n","        cycle_criterion: the cycle consistency loss function; takes the real images from X and\n","                        those images put through a X->Y generator and then Y->X generator\n","                        and returns the cycle consistency loss (which you aim to minimize)\n","    '''\n","    cycle_X = gen_YX(fake_Y)\n","    cycle_loss = cycle_criterion(cycle_X, real_X)\n","    return cycle_loss, cycle_X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwoaceZHlIM-"},"source":["def get_gen_loss(real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, identity_criterion, cycle_criterion, lambda_identity=0.1, lambda_cycle=10): # Lambda_identity:0.1\n","    '''\n","    Return the loss of the generator given inputs.\n","    Parameters:\n","        real_A: the real images from pile A\n","        real_B: the real images from pile B\n","        gen_AB: the generator for class A to B; takes images and returns the images \n","            transformed to class B\n","        gen_BA: the generator for class B to A; takes images and returns the images \n","            transformed to class A\n","        disc_A: the discriminator for class A; takes images and returns real/fake class A\n","            prediction matrices\n","        disc_B: the discriminator for class B; takes images and returns real/fake class B\n","            prediction matrices\n","        adv_criterion: the adversarial loss function; takes the discriminator \n","            predictions and the true labels and returns a adversarial \n","            loss (which you aim to minimize)\n","        identity_criterion: the reconstruction loss function used for identity loss\n","            and cycle consistency loss; takes two sets of images and returns\n","            their pixel differences (which you aim to minimize)\n","        cycle_criterion: the cycle consistency loss function; takes the real images from X and\n","            those images put through a X->Y generator and then Y->X generator\n","            and returns the cycle consistency loss (which you aim to minimize).\n","            Note that in practice, cycle_criterion == identity_criterion == L1 loss\n","        lambda_identity: the weight of the identity loss\n","        lambda_cycle: the weight of the cycle-consistency loss\n","    '''\n","\n","    # Adversarial Loss -- get_gen_adversarial_loss(real_X, disc_Y, gen_XY, adv_criterion)\n","    adv_loss_BA, fake_A = get_gen_adversarial_loss(real_B, disc_A, gen_BA, adv_criterion)\n","    adv_loss_AB, fake_B = get_gen_adversarial_loss(real_A, disc_B, gen_AB, adv_criterion)\n","    gen_adversarial_loss = adv_loss_BA + adv_loss_AB\n","\n","    # Identity Loss -- get_identity_loss(real_X, gen_YX, identity_criterion)\n","    identity_loss_A, identity_A = get_identity_loss(real_A, gen_BA, identity_criterion)\n","    identity_loss_B, identity_B = get_identity_loss(real_B, gen_AB, identity_criterion)\n","    gen_identity_loss = identity_loss_A + identity_loss_B\n","\n","    # Cycle-consistency Loss -- get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion)\n","    cycle_loss_BA, cycle_A = get_cycle_consistency_loss(real_A, fake_B, gen_BA, cycle_criterion)\n","    cycle_loss_AB, cycle_B = get_cycle_consistency_loss(real_B, fake_A, gen_AB, cycle_criterion)\n","    gen_cycle_loss = cycle_loss_BA + cycle_loss_AB\n","\n","    # Total loss\n","    gen_loss = lambda_identity * gen_identity_loss + lambda_cycle * gen_cycle_loss + gen_adversarial_loss\n","\n","    return gen_loss, fake_A, fake_B"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389,"referenced_widgets":["0ffd70aa02d54102a8a86b5c6dee3b08","b3b8bd5e20f343f6b957c015c3e7a397","58096c7220494b4799e7e179ef8775af","f06d9899c18d42a8ac3e3713602fa87b","d7c05acfc1b54839b0c4c9bd06af72bf","39d109915f1a441a8666ab6e2d175eff","398723bac15540da844d922e0356fbe7","a85fa578b47f4ed78955d09ccb11b194"]},"id":"fwEA_bFJlJCV","executionInfo":{"status":"error","timestamp":1617525721403,"user_tz":-330,"elapsed":1558,"user":{"displayName":"VAIDEHI SOM","photoUrl":"","userId":"04027413623674072385"}},"outputId":"9e979762-4efd-499b-accf-2b9b979043a5"},"source":["'''from skimage import color\n","import numpy as np\n","import cv2\n","plt.rcParams[\"figure.figsize\"] = (10, 10)\n","\n","\n","real_A = cv2.imread(\"/content/1.png\", 0)\n","#real_A = nn.functional.interpolate(real_A, size=target_shape)\n","fakeA = gen_BA(pic1)\n","plt.imshow(fake1)\n","\n","pic2 = cv2.imread(\"/content/2.png\", 0)\n","fakeB = gen_AB(pic2)\n","cv2.imshow(fake2)'''\n","\n","from skimage import color\n","import numpy as np\n","plt.rcParams[\"figure.figsize\"] = (5, 5)\n","\n","\n","def train(save_model=False):\n","    #mean_generator_loss = 0\n","    #mean_discriminator_loss = 0\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    cur_step = 0\n","\n","    for epoch in range(n_epochs):\n","        # Dataloader returns the batches\n","        #for image, _ in tqdm(dataloader):\n","        for real_A, real_B in tqdm(dataloader):\n","            # image_width = image.shape[3]\n","            real_A = nn.functional.interpolate(real_A, size=(target_shape1, target_shape2))\n","            real_B = nn.functional.interpolate(real_B, size=(target_shape1, target_shape2))\n","            cur_batch_size = len(real_A)\n","            real_A = real_A.to(device)\n","            real_B = real_B.to(device)\n","\n","            ### Update discriminator A ###\n","            #disc_A_opt.zero_grad() # Zero out the gradient before backpropagation\n","            with torch.no_grad():\n","                fake_A = gen_BA(real_B)\n","            #disc_A_loss = get_disc_loss(real_A, fake_A, disc_A, adv_criterion)\n","            #disc_A_loss.backward(retain_graph=True) # Update gradients\n","            #disc_A_opt.step() # Update optimizer\n","\n","            ### Update discriminator B ###\n","            #disc_B_opt.zero_grad() # Zero out the gradient before backpropagation\n","            with torch.no_grad():\n","                fake_B = gen_AB(real_A)\n","            #disc_B_loss = get_disc_loss(real_B, fake_B, disc_B, adv_criterion)\n","            #disc_B_loss.backward(retain_graph=True) # Update gradients\n","            #disc_B_opt.step() # Update optimizer\n","\n","            ### Update generator ###\n","            #gen_opt.zero_grad()\n","            #gen_loss, fake_A, fake_B = get_gen_loss(\n","            #    real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, recon_criterion, recon_criterion\n","            #)\n","            #gen_loss.backward() # Update gradients\n","            #gen_opt.step() # Update optimizer\n","\n","            # Keep track of the average discriminator loss\n","            #mean_discriminator_loss += disc_A_loss.item() / display_step\n","            # Keep track of the average generator loss\n","            #mean_generator_loss += gen_loss.item() / display_step\n","\n","            ### Visualization code ###\n","            if cur_step % display_step == 0:\n","                #print(f\"Epoch {epoch}: Step {cur_step}: Generator (U-Net) loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n","                print(f\"Epoch {epoch}: Step {cur_step}\")\n","                \n","                ra = show_tensor_images(real_A, size=(dim_A, target_shape1, target_shape2))\n","                ra = torchvision.transforms.ToPILImage()(ra.squeeze_(0))\n","                ra.save(f\"./data_5/67_crop2_{cur_step}_r.jpg\", dpi=(500,500))\n","                \n","                fb = show_tensor_images(fake_B, size=(dim_B, target_shape1, target_shape2))\n","                fb = torchvision.transforms.ToPILImage()(fb.squeeze_(0))\n","                fb.save(f\"./data_5/67_crop2_{cur_step}_f.jpg\", dpi=(500,500))\n","\n","                #mean_generator_loss = 0\n","                #mean_discriminator_loss = 0\n","               \n","                '''if save_model:\n","                    torch.save({\n","                        'gen_AB': gen_AB.state_dict(),\n","                        'gen_BA': gen_BA.state_dict(),\n","                        'gen_opt': gen_opt.state_dict(),\n","                        'disc_A': disc_A.state_dict(),\n","                        'disc_A_opt': disc_A_opt.state_dict(),\n","                        'disc_B': disc_B.state_dict(),\n","                        'disc_B_opt': disc_B_opt.state_dict()\n","                    }, f\"cycleGAN_4_{cur_step}.pth\")'''\n","            cur_step += 1\n","train()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ffd70aa02d54102a8a86b5c6dee3b08","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Epoch 0: Step 0\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-d7198c565fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m                     }, f\"cycleGAN_4_{cur_step}.pth\")'''\n\u001b[1;32m     98\u001b[0m             \u001b[0mcur_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-39-d7198c565fdf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(save_model)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}: Step {cur_step}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m#ra = transforms.CenterCrop(size=(target_shape1, target_shape2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_tensor_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, size=(dim_A, target_shape1, target_shape2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_shape1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_shape2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-693e7c06ccdb>\u001b[0m in \u001b[0;36mshow_tensor_images\u001b[0;34m(image_tensor, num_images, size)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mimage_shifted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mimage_unflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_shifted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m#img = image_unflat[:num_images]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mimage_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_unflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1, 28, 28]' is invalid for input of size 69120"]}]},{"cell_type":"code","metadata":{"id":"jKKLtyKDsfcg"},"source":[""],"execution_count":null,"outputs":[]}]}